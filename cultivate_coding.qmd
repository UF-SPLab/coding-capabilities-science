---
title: "Learn to Code and </br>Excel in Your Scientific Endeavors"
subtitle: "Cultivate capabilities in neuro-data analytics and</br> high-performance statistical computing"
author: "Ryan Mears"
format: 
  revealjs:
    slide-number: true
    footnotes-hover: true
editor: visual
---

# Why Code? 


::: r-fit-text
> Your closest\
> collaborator is\
> you six months ago\
> but you don't reply to email.
:::

-   

    ### **Karl Broman**

## Reasons to Learn to Code

-   Adaptability

-   Flexibility

-   Integration

-   Repeatability

-   Reusability

-   Sharability


## Code, How?

::: incremental
-   *Adapt* example code directly from online documentation
-   *Flexibly* accommodate new datatypes & formats
-   *Integrate* extensive additional code libraries & solutions from others
-   *Repeat* analysis with additional data or with changes to initial steps for reanalyzing the same data
-   *Reuse* with minor modifications for similar data
-   *Share* with others so they can modify, repeat, reuse, and build on code
:::

------------------------------------------------------------------------

## Return on Investment for Code {.smaller}

| Advantages                      | Limitations                         |
|---------------------------------|-------------------------------------|
| High Customization              | Increased User Knowledge Dependence |
| High Composability              | Reduced Usability for Novices                   |
| Graphical Resource Independence | Explicit Command Requirement        |
| Integration & Modularity        |                                     |
| Utility of Scale                |                                     |
| Searchability                   |                                     |

------------------------------------------------------------------------

## Code Libraries & APIs {.smaller}

-   **Code Libraries**: collections of pre-written code that can be used to perform tasks.
- **APIs** (Application Programming Interfaces): rules & protocols allowing users or other applications to interact with a software application.

::: columns


:::: {.column width="50%"}

#### GUI (Graphical User Interface)

- Intuitiveness and Accessibility
- Limited to Pre-defined Options (i.e., graphical elements)
- API is rigidly structured & hidden
- Event-Driven Programming


::::

:::: {.column width="50%}

#### CLI (Command Line Interface)

- High Flexibility and Control
- Scripting and Automation
- Steep Learning Curve 
- Efficiency for Experienced Users

::::
:::




:::{.notes}

Ways in which users or computers interact with APIs can vary significantly between GUIs and CLIs, reflecting differences in usability, flexibility, and the type of user experience they are designed to offer.

GUI

1. **Intuitiveness and Accessibility:** 
GUIs are designed to be intuitive and accessible to users, offering graphical elements such as buttons, icons, and menus to interact with. The API in the context of a GUI might be more about how these elements are programmed to interact with the software's logic and data. Users don't usually interact with the API directly but through these graphical elements.

2. **Limited to Pre-defined Options:** GUIs often limit the user to predefined operations or workflows designed by the application developers. The API interactions are therefore constrained by what the GUI offers, making it potentially less flexible but simpler for non-technical users.

3. **Event-Driven Programming:** GUI APIs are heavily reliant on event-driven programming, where code execution is triggered by user actions like clicks, drags, and drops. This model affects how APIs are structured and utilized in GUI applications, focusing on responsiveness and user interaction feedback.


CLI

1. **Flexibility and Control:** CLI provides a more direct and text-based way to interact with software, offering greater flexibility and control to the user. Users can combine commands in scripts, automate tasks, and utilize the full breadth of the API's capabilities. This makes CLIs particularly favored by developers and system administrators.

2. **Scripting and Automation:** CLI allows for scripting and automation. Users can write scripts that directly call an application's API functions, enabling complex workflows and batch processing. This aspect is less inherent in GUIs, which are more designed for interactive use.

3. **Steep Learning Curve:** The increased power and flexibility of CLIs come with a steeper learning curve. Users must be familiar with the command syntax and the software's API to effectively use it. This makes CLIs less accessible to non-technical users compared to GUIs.

4. **Efficiency for Experienced Users:** For users who are comfortable with command-line interfaces, CLIs can offer a more efficient way to interact with software. Operations that might require several clicks in a GUI can often be performed with a single command in a CLI.

:::

# Statistics & Data Science

## Statistics & Data Science Curricula {.smaller}

::: columns
:::: {.column width="30%"}

**Programming** 

  - Structured 
  - Efficiency 
  - HPC 

**Data Formats** 

  - Ragged arrays 
  - Text data 
  - Data cleaning 
  
::::
:::: {.column width="30%"}

**Data Tech**  

  - RDBMS (SQL) 
  - RegEx 
  - XML 
  - Shell commands 
  - Web scraping 
	

  
::::
:::: {.column width="30%"}


  
**Work Flow**

  - Reproducibility 
  - Web publishing 
  - Revision control 
	 
**Statistical**	

  - Simulations 
  - Modern methods 
  - Visualization 
  
*From Hardin et al.,2015,*^[Hardin, J., Hoerl, R., Horton, N. J., Nolan, D., Baumer, B., Hall-Holt, O., ... & Ward, M. (2015). Data science in statistics curricula: Preparing students to “think with data”. The American Statistician, 69(4), 343-353. [http://dx.doi.org/10.1080/00031305.2015.1077729](http://dx.doi.org/10.1080/00031305.2015.1077729)]
::::
:::



## Data Science Activities {.smaller}

::: columns
:::: {.column width="50%"}

1. Data Gathering, Preparation, and Exploration
2. Data Representation and Transformation
3. Computing with Data
4. Data Modeling
5. Data Visualization and Presentation
6. Science about Data Science



*From Donoho, 2017.*^[Donoho, D. (2017). 50 years of data science. Journal of</br> Computational  and  Graphical Statistics, 26(4), 745-766.</br>  [https://doi.org/10.1080/10618600.2017.1384734](https://doi.org/10.1080/10618600.2017.1384734)]



::::
:::: {.column width="50%"}

Ismay, C., & Kim, A. Y. (2019). Statistical inference via data science: a ModernDive into R and the tidyverse. Chapman and Hall/CRC. [https://moderndive.com/](https://moderndive.com/)

Navarro, D. (2015). Learning statistics with R: A tutorial for psychology students and other beginners (version 0.6). University of New South Wales. [https://learningstatisticswithr.com/](https://learningstatisticswithr.com/)

Donoghue T, Voytek B, & Ellis S (2022). Course Materials for Data Science in 
Practice. Journal of Open Source Education, 5(51), 121. [https://doi.org/10.21105/jose.00121](https://doi.org/10.21105/jose.00121)

::::
:::


# Scientific Data Analysis Workflows

## Conventional Data Processing 


::: columns
:::: {.column width="25%"}

::::: {style="display: flex;"}
<div>

**SPSS & MS Office**

</br>

**Scripting in </br> Matlab/Python**

</div>
:::::

::::
:::: {.column width="70%"}


```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=2}

%%{init: {'theme':'forest'}}%%

graph LR
    A[Raw Data] -->|Import </br> </br> </tab> | B(Excel)
    B -->|Process and </tab> </br> </br> </tab> Analyze| C(SPSS)
    C -->|Present| D(PowerPoint)
    C -->|Document| E(Word)
    

    classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
    classDef orange fill:#f96,stroke:#333,stroke-width:1px;
    classDef white fill:#fff,stroke:#333,stroke-width:1px; 
    classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
    classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
    classDef purple fill:#6A2A60,stroke:#FFF,stroke-width:1.5px;
    classDef royalblue fill:#0021A5,stroke:#FFF,stroke-width:1.5px;
    
    class B,C green
    class A red
    class E royalblue
    class D purple

    
```


```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=2}

%%{init: {'theme':'forest'}}%%

graph LR

    A[Raw Data] -->|Import </br> </br> </br> | B(IDE)
    B -->|Process </br> </br> </tab> | C(IDE)
    C -->|Analyze| B
    C -->|Present| D(PowerPoint)
    C -->|Document| E(Word)
    

    classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
    classDef orange fill:#f96,stroke:#333,stroke-width:1px;
    classDef white fill:#fff,stroke:#333,stroke-width:1px; 
    classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
    classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
    classDef purple fill:#6A2A60,stroke:#FFF,stroke-width:1.5px;
    classDef royalblue fill:#0021A5,stroke:#FFF,stroke-width:1.5px;
    
    class B,C green
    class A red
    class E royalblue
    class D purple
    
    
```


::::
:::




# Open Science 

Promotion of Scientific Transparency & Reliability  
Presents Advantages for Research Training

## Open Science {.smaller}

![](RR-Open2021.jpg){width=75%}

Garrett-Rufin, et al, 2021^[Garrett-Ruffin, S., Hindash, A. C., Kaczkurkin, A. N., Mears, R. P., Morales, S., Paul, K., ... & Keil, A. (2021). Open science in psychophysiology: An overview of challenges and emerging solutions. International Journal of Psychophysiology, 162, 69-78.]


:::{.notes}
Fig. 1. Open science practices affect the research process at multiple levels.

The process of experimental research, involving steps ranging from hypothesis generation to drawing conclusions, is positively affected by various open science practices such as pre-registration and multi-laboratory studies. Direct replication requires sequential repetition of measurements and treatments. In a multisite study, identical measurements and treatments are carried out simultaneously between multiple similar experiment settings. Replicability in a multisite study thus supports the robustness of study outcomes. In this context, Computational Reproducibility addresses *Researcher Degrees of Freedom by constraining the influences of user defined parameters, code, and computing environment on analysis outcome. Likewise, preregistration precludes questionable research practices such as HARKing (hypothesizing after the results are known) by eliminating the possibility of outcome-dependent decision making.
:::


## Un/Supervised Machine Learning {.smaller}

- Many advantages for dimensionality reduction, clustering, and classification

- **Unsupervised Learning**: No labels are provided, the algorithm tries to learn the structure of the data

- **Supervised Learning**: Labels are provided, the algorithm tries to learn the mapping function from the input to the output labels

- **Reinforcement Learning**: The algorithm learns to perform an action from experience

## Machine Learning Workflow {.smaller}

- Open Science practices can be integrated into the machine learning workflow to ensure reproducibility and transparency

- Most machine learning models are blackbox models, which makes it difficult to understand the model's decision-making process

- Machine Learning models are data-driven, which means that the model's performance is highly dependent on the quality of the data

- Machine Learning model workflows are complex and involve many steps, such as data preprocessing, feature selection, model selection, hyperparameter tuning, and model evaluation

- Machine Learning model frameworks involve declarative/implicit programming style for selection of models and hyperparameters

- However, reproducibility is a major challenge in machine learning because of the blackbox nature of most models, the complexity of the models, and the large number of hyperparameters that need to be tuned

- **Computational Reproducibility** is more essential than ever to ensure the reproducibility of machine learning models



## Computational Reproducibility {.smaller}

::: columns
:::: {.column width="60%"}

::: {style="display: flex;"}

<div>
</br>
 
```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=1.5, fig.height=12}

graph TD;
      A[coarse] ----->|Specificity </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab>   | B[granular]


```

</div>

::: {.middle style="text-align: center"}
```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=9, fig.height=2}


graph LR;
    A[Abstract] ---> |Representativeness </br> </br> </tab> | B[Concrete]


```

+--------------------------------------------------+------------------------------------------------------+
| [**Computing Environment**]{style="color: red;"} |[**Workflow**/</br>**Pipeline**]{style="color: gray;"}|
+--------------------------------------------------+------------------------------------------------------+
| [**Raw Data** </br> & Code]{style="color: gray;"}| [**Analysis Derivatives** </br> - *derived variables* </br> - *stats/plots*]{style="color: gray;"} |
+--------------------------------------------------+------------------------------------------------------+

: {.table .table-bordered .text-white .border .border-white .rounded-3 "}
:::
:::




::::
:::: {.column width="40%"}


</br> </br> 



-   [ ] learn how computing works (computing abstractions, operating systems graphics hardware, applications/software, filesystems, etc.)
-   [ ] learn shell scripting for generalizing tasks across hardware/operating systems





::::
:::


## Computational Reproducibility {.smaller}

::: columns
:::: {.column width="60%"}

::: {style="display: flex;"}

<div>
</br>

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=1.5, fig.height=12}

graph TD;
      A[coarse] ----->|Specificity </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab>   | B[granular]


```

</div>

</br>

::: {.middle style="text-align: center"}
```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=9, fig.height=2}


graph LR;
    A[Abstract] ---> |Representativeness </br> </br> </tab> | B[Concrete]


```


+--------------------------------------------------+------------------------------------------------------+
| [**Computing Environment**]{style="color:gray;"} |[**Workflow**/</br>**Pipeline**]{style="color:  red;"}|
+--------------------------------------------------+------------------------------------------------------+
| [**Raw Data** </br> & Code]{style="color: gray;"}| [**Analysis Derivatives** </br> - *derived variables* </br> - *stats/plots*]{style="color: gray;"} |   
+--------------------------------------------------+------------------------------------------------------+

: {.table .table-bordered .text-white .dark .border .border-white .rounded-3"}
:::
:::




::::
:::: {.column width="40%"}



</br> </br> 



-   [ ] reproducibility/open science



::::
:::




## Computational Reproducibility {.smaller}

::: columns
:::: {.column width="60%"}

::: {style="display: flex;"}

<div>

</br>

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=1.5, fig.height=12}

graph TD

      A[coarse] ----->|Specificity </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab>   | B[granular]


```

</div>

::: {.middle style="text-align: center"}
```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=9, fig.height=2}


graph LR;
    A[Abstract] ---> |Representativeness </br> </br> </tab> | B[Concrete]


```

+--------------------------------------------------+------------------------------------------------------+
| [**Computing Environment**]{style="color: gray;"}|[**Workflow**/</br>**Pipeline**]{style="color: gray;"}|
+--------------------------------------------------+------------------------------------------------------+
| [**Raw Data** </br> & Code]{style="color: red;"} | [**Analysis Derivatives** </br> - *derived variables* </br> - *stats/plots*]{style="color: gray;"} |   
+--------------------------------------------------+------------------------------------------------------+

: {.table .table-bordered .text-white .border .border-white .rounded-3 }
:::
:::





::::
:::: {.column width="40%"}


</br> </br> 



-   [ ] learn how to use this
-   [ ] dataframe concepts

::::
:::


## Computational Reproducibility {.smaller}

::: columns
:::: {.column width="60%"}

::: {style="display: flex;"}

<div>

</br>

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=1.5, fig.height=12}
graph TD

      A[coarse] ----->|Specificity </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab>   | B[granular]


```

</div>

::: {.middle style="text-align: center"}
```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=9, fig.height=2}


graph LR;
    A[Abstract] ---> |Representativeness </br> </br> </tab> | B[Concrete]


```

+---------------------------------------------------+-------------------------------------------------------+
| [**Computing Environment**]{style="color: gray;"} |[**Workflow**/</br>**Pipeline**]{style="color: gray;"} |
+---------------------------------------------------+-------------------------------------------------------+
| [**Raw Data** </br> & Code]{style="color: gray;"} | [**Analysis Derivatives** </br> - *derived variables* </br> -*stats/plots*]{style="color: red;"} |   
+---------------------------------------------------+-------------------------------------------------------+

: {.table .table-bordered .text-white .border .border-white .rounded-3 }
:::
:::





::::
:::: {.column width="40%"}

</br> </br> 



-   [ ] data wrangling in Python/R
-   [ ] learn how to graph in Python/R
-   [ ] implementation of intermediate steps
-   [ ] inferential statistics


::::
:::




## Workflows {.smaller}

::: columns
:::: {.column width="25%"}

::::: {style="display: flex;"}
<div>

</br>

**SPSS & MS Office**

</br></br>

**Scripting in </br> Matlab / Python**

</br>

**Hybrid Interactive Python / Matlab**

</br></br>

**R & RStudio Workflow**

</div>
:::::

::::
:::: {.column width="70%"}


```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=2}

%%{init: {'theme':'forest'}}%%

graph LR
    A[Raw Data] -->|Import </br> </br> </tab> | B(Excel)
    B -->|Process and </tab> </br> </br> </tab> Analyze| C(SPSS)
    C -->|Present| D(PowerPoint)
    C -->|Document| E(Word)
    

    classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
    classDef orange fill:#f96,stroke:#333,stroke-width:1px;
    classDef white fill:#fff,stroke:#333,stroke-width:1px; 
    classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
    classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
    classDef purple fill:#6A2A60,stroke:#FFF,stroke-width:1.5px;
    classDef royalblue fill:#0021A5,stroke:#FFF,stroke-width:1.5px;
    
    class B,C green
    class A red
    class E royalblue
    class D purple

    
```


```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=2}

%%{init: {'theme':'forest'}}%%

graph LR

    A[Raw Data] -->|Import </br> </br> </br> | B(IDE)
    B -->|Process </br> </br> </tab> | C(IDE)
    C -->|Analyze| B
    C -->|Present| D(PowerPoint)
    C -->|Document| E(Word)
    

    classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
    classDef orange fill:#f96,stroke:#333,stroke-width:1px;
    classDef white fill:#fff,stroke:#333,stroke-width:1px; 
    classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
    classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
    classDef purple fill:#6A2A60,stroke:#FFF,stroke-width:1.5px;
    classDef royalblue fill:#0021A5,stroke:#FFF,stroke-width:1.5px;
    
    class B,C green
    class A red
    class E royalblue
    class D purple
    
    
```


```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=2}

%%{init: {'theme':'forest'}}%%

graph LR
    A[Raw Data] -->|Import </br> </br> </tab> | B(Jupyter Notebook)
    B -->|Process and </tab> </br> </br> </tab> Analyze| C(Jupyter Notebook)
    C -->|Present| D(PowerPoint)
    C -->|Document| E(Word)
    

    classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
    classDef orange fill:#f96,stroke:#333,stroke-width:1px;
    classDef white fill:#fff,stroke:#333,stroke-width:1px; 
    classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
    classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
    classDef purple fill:#6A2A60,stroke:#FFF,stroke-width:1.5px;
    classDef royalblue fill:#0021A5,stroke:#FFF,stroke-width:1.5px;
    
    class B,C green
    class A red
    class E royalblue
    class D purple
    
    
```


```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=2}

    
%%{init: {'theme':'forest'}}%%

graph LR

      subgraph RStudio
      
            B[RStudio]-->|Process</tab> </br> </br> </tab>| C[RStudio]
            C -->| Visualize</tab> </br> </br> </tab> | D[RStudio]
            D -->| Analyze</tab> </br> </br> </tab> | B

        end

      subgraph Output
            D --> F[Documents /</br>Slides /</br>Dashboards /</br>Websites]
        end
        
      subgraph Input
            A[Raw Data] -->|Import </br> </br> </tab> | B
        end
 
    
    classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
    classDef orange fill:#f96,stroke:#333,stroke-width:1px;
    classDef white fill:#fff,stroke:#333,stroke-width:1px; 
    classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
    classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
    classDef purple fill:#6A2A60,stroke:#FFF,stroke-width:1.5px;
    classDef royalblue fill:#0021A5,stroke:#FFF,stroke-width:1.5px;
    
    class RStudio,Input,Output white
    class B,C green
    class A red
    class D royalblue
    class F purple

    
```

::::
:::


## Video Behavior Analysis Workflow {.smaller}



```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=3}

%%{init: {'theme':'forest'}}%%

graph LR

    subgraph Analysis
       s6 -->  s7{Inferential Stats}
    end
 
    subgraph ETHZ-INS/DLCAnalyzer
        s3 --> s4[Data Wrangling </br> Graphing]
        s5 --> s6[Data Wrangling]
        s6 --> s4
        
        subgraph dlca[</tab> </br> Derivative Variables / Data Visualizations]
          s4 --> s5[unsupervised</br> classification</br> and clustering]
        end
    end

    subgraph DeepLabCut

        subgraph Training
            s1[add keypoints]-->s2[run training]
            s2 --> s2a[analyze</br> training]
            s2a -->s2

        end

        subgraph Decoding
            s2a --> s3a[add</br> videos] 
            
        end
        
         
        s3a --> s3[import data</br>transform]
       
    end
    
    subgraph Read-in-Data[Raw Data]
        s0[import video] --> s1
    end
    
 %% Notice that no text in shape are added here instead that is appended further down
    

    %% Comments after double percent signs

     classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
     classDef orange fill:#f96,stroke:#333,stroke-width:1px;
     classDef white fill:#fff,stroke:#333,stroke-width:1px;
     classDef sq stroke:#f66,stroke-width:1px;
     classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
     classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
     class sq,Analysis green
     class ETHZ-INS/DLCAnalyzer orange
     class Training,dlca,s0 white
     class DeepLabCut white
     class Decoding blue
     class Read-in-Data red
     
```


##

::: columns
:::: {.column width="50%"}

::::
:::: {.column width="50%"}

::::
:::

## MNE-BIDS-Pipeline {.smaller}

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE}

%%{init: {'theme':'forest'}}%%

graph LR;
        subgraph Sensor-level-analysis

       A[Decode time-by-time </br>  using a 'sliding' estimator] --> B[Time-frequency </br> decomposition] 
       B --> C[Decoding </br> based on </br> common spatial patterns] 
       C --> D[Noise covariance </br> estimation] 
       D --> E[Group average </br> at the </br> sensor level] 
   end
  
    subgraph Preprocessing

       l1[Assess </br> channel-wise </br> data quality] --> l2[Estimate </br> head positions] 
       l2 --> l3[Apply low- and </br> high-pass filters] 
       l3 --> l4[Temporal regression </br> for artifact removal]
       l4 --> l5a[Fit ICA] 
       l5a --> l6a[Find ICA </br> artifacts] 
       l4 --> l5b[Extract epochs] 
       l5b --> l7[Apply ICA] 
       l6a --> l7 --> l8[Remove epochs </br> based on  </br> PTP amplitudes]  
       l5b --> l6b[Apply SSP] 
       l4 --> l5c[Compute SSP] 
       l5c --> l6b[Apply SSP] 
       l6b --> l8[Remove epochs </br> based on </br> PTP amplitudes] 
       l8 --> l9[Extract </br> evoked data for </br> each condition] 
       l9 --> l10[Decode pairs of </br>  conditions based on </br> entire epochs] 
    end
    

   
   %%l0[Raw Data] --> l1
   %%E --> F[Output </br> Statistical </br> Analysis Pipeline]    
   
   classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
   classDef orange fill:#f96,stroke:#333,stroke-width:1px;
   classDef white fill:#fff,stroke:#333,stroke-width:1px;
   classDef sq stroke:#f66,stroke-width:1px;
   classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
   classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
     
   class Preprocessing,Sensor-level-analysis white

```

# MNE-BIDS-pipeline {.smaller}

1.  Prepare your dataset
2.  Create a configuration file
3.  Run the pipeline

## Prepare your dataset

MNE-BIDS-Pipeline only works with BIDS-formatted raw data. To find out more about BIDS and how to convert your data to the BIDS format, please see the documentation of MNE-BIDS.

![](https://mne.tools/mne-bids/assets/MNE-BIDS.png)

## Create a configuration file {.smaller}

All parameters of the pipeline are controlled via a configuration file. Create a template configuration file by running the following command:

`mne_bids_pipeline --create-config=/path/to/custom_config.py`

```{python, eval=FALSE}
import numpy as np

study_name = "ds000247"
bids_root = f"~/mne_data/{study_name}"
deriv_root = f"~/mne_data/derivatives/mne-bids-pipeline/{study_name}"

subjects = ["0002"]
sessions = ["01"]
task = "rest"
task_is_rest = True

crop_runs = (0, 100)  # to speed up computations

ch_types = ["meg"]
spatial_filter = "ssp"

l_freq = 1.0
h_freq = 40.0

rest_epochs_duration = 10
rest_epochs_overlap = 0

epochs_tmin = 0
baseline = None

```

-   [from example ds000247](https://mne.tools/mne-bids-pipeline/stable/examples/ds000247.html#configuration)

## Run the pipeline {.smaller}

To run the full pipeline, simply call:

`mne_bids_pipeline --config=/path/to/your/custom_config.py`

To run part of the pipeline, you can specify the stage you want to run:

-   Run only the preprocessing steps:
    -   `mne_bids_pipeline --config=/path/to/your/custom_config.py --steps=preprocessing`

## Optional Parts of the pipeline {.smaller}

-   Run only the sensor-level processing steps:
    -   `mne_bids_pipeline --config=/path/to/your/custom_config.py --steps=sensor`
-   Run only the source-level (inverse solution) processing steps:
    -   `mne_bids_pipeline --config=/path/to/your/custom_config.py --steps=source`
-   (Re-)run ICA:
    -   `mne_bids_pipeline --config=/path/to/your/custom_config.py --steps=preprocessing/ica`
-   You can also run multiple steps with one command by separating different steps by a comma. For example, to run preprocessing and sensor-level processing steps using a single command, do:
    -   `mne_bids_pipeline --config=/path/to/your/custom_config.py --steps=preprocessing,sensor`

# Running the pipeline on HiPerGator {.smaller}

## Log on to HiPerGator terminal/shell {.smaller}

Connect to UF Research Computing HPG3 Cluster from terminal on macOS

Type the following command in the terminal:

`ssh` *urgatoruser*`@hpg.rc.ufl.edu`

Then, follow the prompts to enter your password and Duo two-factor authentication.

**Your password will not be displayed as you type it.** Then, your terminal should look like this:

``` bash
ssh urgatoruser@hpg.rc.ufl.edu
Password: xxxxxxxx
Duo two-factor login for urgatoruser@ufl.edu

Enter a passcode or select one of the following options:

 1. Duo Push to XXX-XXX-1809
 2. Phone call to XXX-XXX-1809

Passcode or option (1-2): 1
```

Works through the UF Single-sign on similar to for Canvas and gatormail.

## Start a compute node on HiPerGator {.smaller}

-   single node 2 CPU core job with 2gb of RAM for 90 minutes can be started with the following command

``` bash
srun --account=psb4934 --qos=psb4934 --ntasks=1 --cpus-per-task=2 --mem=2gb -t 90 --pty bash -i
```

`[mygatoruser@login1 ~]$ srun --pty -p hpg2-compute -n 1 -N 1 -t 90 --mem=2gb /bin/bash`

-   single node 4 CPU core job with 28gb of RAM for 120 minutes can be started with the following command

``` bash
srun --account=psb4934 --qos=psb4934 --ntasks=1 --cpus-per-task=4 --mem=28gb -t 60 --pty bash -i
```

## Check that your job is running

Now you are on the compute node. You can check the hostname and the node list with the following commands:

``` bash
echo "Hello from $(hostname)"
```

`Hello from c0711a-s6.ufhpc`

``` bash
echo $SLURM_JOB_NODELIST
```

`c0711a-s6`

## Activate: Conda-env for pipeline {.smaller}

Navigate to group storage on Blue drive then activate conda environment to have access to pipeline

``` bash
cd /blue/psb4934/share/mears/misophonia/miso-project-pipeline

module load conda
conda activate mne_v1_6
```




## R & RStudio Workflow

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=2}


%%{init: {'theme':'forest'}}%%

graph LR

    A[Raw Data] -->|Import </br> </br> </tab> | C[RStudio]
    
      subgraph RStudio
            direction LR
            B[RStudio]-->|Process</tab> </br> </br> </tab>| C
            C -->| Analyze</tab> </br> </br> </tab> | B
            

        end


  B -->|Present and </tab> </br> </br> </tab> Document| E[Documents/Slides/Dashboards/Websites]
  
    classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
    classDef orange fill:#f96,stroke:#333,stroke-width:1px;
    classDef white fill:#fff,stroke:#333,stroke-width:1px; 
    classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
    classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
    classDef purple fill:#6A2A60,stroke:#FFF,stroke-width:1.5px;
    classDef royalblue fill:#0021A5,stroke:#FFF,stroke-width:1.5px;
    
    class RStudio white
    class B,C green
    class A red
    class D royalblue

```

# Open Science: Finding and Sharing Analysis Code

## Version Control & Open Software


## Why use Git?

- Remote repo: accessed & <br> updated anytime
- Solutions for continuous <br> merging of sets of changes

![](team-git.png){.absolute top=0 right=-10  height="450"} 

::: aside
> Perez-Riverol, *et al.* (2016). Ten Simple Rules for Taking Advantage of Git & GitHub. *PLoS computational biology*, [12(7), e1004947.](https://doi.org/10.1371/journal.pcbi.1004947)
::: 


## Key Feature of Git: Timeline Control 

![](reachability-example.png)

[Example from *Think Like (a) Git: A Guide for the Perplexed*](https://think-like-a-git.net/sections/graph-theory/reachability.html)

::: {.notes}

You can think of this graph as a set of three parallel universes with time flowing from left to right, so that A is the beginning of recorded history. (The arrow represents the "follows" or "is subsequent to" relationship, so you might say that "B follows A".)

:::

## Key Feature of Git: Timeline Control 

![](reachability-exampleE.jpg)


## Key Feature of Git: Timeline Control 

![](reachability-exampleK.jpg)

::: {.notes}

- If you start from E, the history you'll see is A, B, C, D, E.

:::

## Key Feature of Git: Timeline Control 

![](reachability-exampleH.jpg)


::: {.notes}
- If you start from K, the history you'll see is A, B, C, I, J, K.
But the really important thing about this is that no matter which node you start with, some **parts of the graph will be unreachable** to you.  

*turn it around*: **Depending** on where you start, you can **reach parts** of the graph that you couldn't get to otherwise.  

:::


## Key Feature of Git: Timeline Control 

![](graph.png)

## {background-image="Git-4yrs-up.png" background-size="cover"}


::: {.notes}
[Git-4yrs-up 1h 31m](https://youtu.be/1ffBJ4sVUb4?t=5460)
:::


## Free Software on GitHub for Psychology & Neuroscience {.smaller}

::: {.columns}
:::: {.column width="30%"}

**Python Frameworks**

- [Anaconda](https://github.com/ContinuumIO)
- [PyPi](https://pypi.org/)[**(Python Package Index)**]{style="font-size:75%"}
- [Jupyter](https://github.com/jupyter/)/[Pyodide](https://github.com/pyodide/pyodide)
- [Numpy](https://github.com/numpy/numpy) / [Scipy](https://github.com/scipy/scipy)
- [Pandas](https://github.com/pandas-dev/pandas)/[Statsmodels](https://github.com/statsmodels/statsmodels)
- [Matplotlib](https://github.com/matplotlib/matplotlib)/[Seaborn](https://github.com/seaborn)
- [Keras](https://github.com/keras-team/keras)/[TensorFlow](https://github.com/tensorflow/tensorflow)
- [PyTorch](https://github.com/pytorch/pytorch)
- [Scikit-learn](https://github.com/scikit-learn/scikit-learn)


::::

:::: {.column width="40%"}
**R**

- [Tidyverse R](https://github.com/tidyverse)
- [ROpenSci](https://github.com/ropensci)
- CRAN Task Views </br> [**(Comprehensive R Archive Network)**]{style="font-size:75%"}
   - [Psychometrics](CRAN Task View: Psychometric Models and Methods)
   - [Bayesian Inference](https://cran.r-project.org/web/views/Bayesian.html)
   - [Reproducible Research](https://cran.r-project.org/web/views/ReproducibleResearch.html)
   - [Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html)
   - [Teaching Statistics](https://cran.r-project.org/web/views/TeachingStatistics.html)

   
::::

:::: {.column width="30%"}
**Psychology & Neuroscience**  

Python  

- [MNE](https://github.com/mne-tools)
- [PsychoPy](https://www.psychopy.org/)
- [DeepLabCut](https://github.com/DeepLabCut/DeepLabCut)
- [Movement](https://github.com/neuroinformatics-unit/movement)

R  

- [DLCAnalyzer](https://github.com/ETHZ-INS/DLCAnalyzer)
- [qualtRics](https://github.com/ropensci/qualtRics)

::::

:::

:::{.notes}


:::

# Data Science Skills Essential to Machine Learning

## SPSS Transformation & Manipulation {.smaller}

:::{.fragment .aside}
 </br> </tab> </br>  1- Select "variables-to-cases" </br> 2- Select "one" var to restructure </br> 3- Select ID var, 4 vars to transpose, & 1 var to not change </br> 4- Select "one" var to create 

:::


::: {.r-stack}
![](spss_restr1-2.png){.fragment .absolute top=50 left=0 width=90%}
![](spss_restr3-4.png){.fragment .absolute top=100 left=40 width=90%}


:::


:::{.notes}


- Cleophas, T. J., Zwinderman, A. H., Cleophas, T. J., & Zwinderman, A. H. (2020). Restructure Data Wizard for Data Classified the Wrong Way (20 Patients). *Machine Learning in Medicine–A Complete Overview*, 117-121.

- Ravand, H. (2019). Item response theory using hierarchical generalized linear models. *Practical Assessment, Research, and Evaluation, 20(1)*, 7.

- Field, A. (2013). Chapter 20 Multilevel linear models. Discovering statistics using IBM SPSS statistics: Sage Publications Ltd.

:::



## SPSS Transformation & Manipulation {.smaller}

:::{.fragment .aside}
  </br>   </tab>  </br> 5- Select "Sequential Numbers" and name index var as "Time", </br> 6- Select "Options" </br> 7- Select "restructure and/or syntax" and "Finish"

:::

![](spss_restr1-2.png){.absolute top=50 left=0 width=90%}
![](spss_restr3-4.png){.absolute top=100 left=40 width=90%}

::: {.r-stack}

![](spss_restr5-6.png){.fragment .absolute top=150 left=80 width=90%}

![](spss_restr7.png){.fragment .absolute top=200 right=-20 width=45%}

:::


:::{.notes}


- Cleophas, T. J., Zwinderman, A. H., Cleophas, T. J., & Zwinderman, A. H. (2020). Restructure Data Wizard for Data Classified the Wrong Way (20 Patients). *Machine Learning in Medicine–A Complete Overview*, 117-121.

- Ravand, H. (2019). Item response theory using hierarchical generalized linear models. *Practical Assessment, Research, and Evaluation, 20(1)*, 7.

- Field, A. (2013). Chapter 20 Multilevel linear models. Discovering statistics using IBM SPSS statistics: Sage Publications Ltd.

:::




## Transformation & Manipulation in R {.smaller}

```{r}
library(tidyverse)
library(haven)

honeymoon_period <- haven::read_sav("honeymoon_period.sav")

honeymoon_period2 <- head(honeymoon_period,5)
knitr::kable(honeymoon_period2)

# knitr::kable(honeymoon_period2, col.names = gsub("atisfaction", "",names(honeymoon_period2)))

```

```{r, echo=TRUE}
#| code-line-numbers: "2|3-4"
 
honeymoon_period_longer <- honeymoon_period %>% 
  pivot_longer(cols = starts_with("Satisfaction_"), 
               names_to = "Life_satisfaction", 
               values_to = "Time")
```

```{r}
knitr::kable(honeymoon_period_longer)
```


## Tabular Data Frames & Cognitive Load {.smaller}


## {.center}

> Programming for humans not machines 
>  
> - Hal Abelson


# Tidyverse R

```{latex}
                                          .---> Communicate
read -> wrangle ---.--> transform ---+-- /
                   ^                 |            
                   |                 v
                   model <-- visualize 
                   
```

## Tidyverse Concepts

![](tidydata_1.jpg)

## Tidyverse Concepts

![](tidydata_2.jpg)

## Tidyverse Concepts

![](tidydata_3.jpg)

## Tidyverse Concepts

- Many operations are essentially the same as SQL
   - Algorithms are based on set theory, relational algebra, and relational calculus
   - SQL is a declarative language, R and tidyverse R is an imperative/functional language
- Methods for data wrangling are more flexible and powerful than SQL
- Similar to SQL but without the focus on explicit predefined referencing and contraints of database design
- Data Frames are the primary data structure

## Tabular Data in Python & Matlab {.smaller}

- Pandas in Python
   - Data Frames
      - Data Wrangling
         - `read_csv()`
         - `sort_values()`
         - `sort_index()`
         - `reset_index()`
         - `set_index()`
         - `loc[]`
         - `iloc[]`
         - `at[]`
         - `iat[]`
         - `isin()`
         - `query()`
         - `melt()`
         - `stack()`
         - `unstack()`

   - Split-Apply-Combine
         - `groupby()`
         - `pivot_table()`
         
   - Column-based computation
         - `eval()`
         - `assign()`
         - `pipe()`
         - `apply()`
         - `applymap()`
         - `map()`
         
   - DataFrame Joins
         - `merge()`
         - `join()`
         - `concat()`
         
- Data Tables in Matlab
   - Data Tables
   - Split-Apply-Combine
   - Column-based computation
   - Table Joins

:::{.notes}

- `cut()`, `qcut()`, `to_datetime()`, `to_numeric()`,
- `to_timedelta()`, `str()`, `dt()`, `cat()`, `crosstab()`, `corr()`
- `cov()`, `rolling()`, `expanding()`, `ewm()`, `shift()`, `diff()`,
- `pct_change()`, `cumsum()`, `cumprod()`, `cummax()`, `cummin()`, `resample()`, 
- `asfreq()`, `interpolate()`, `fillna()`, `dropna()`, `replace()`,


:::

# How to Develop Coding Capabilities

## Start & Practice

## UFRC Courses & Community Workshops

- ![https://help.rc.ufl.edu/doc/UFRC_Help_and_Documentation](https://help.rc.ufl.edu/doc/UFRC_Help_and_Documentation)
- ![https://practicumai.org/](https://practicumai.org/)
- ![Carpentries: R for Reproducible Scientific Analysis](https://swcarpentry.github.io/r-novice-gapminder/)
- ![Carpentries: Functional Neuroimaging Analysis in Python](https://carpentries-incubator.github.io/SDC-BIDS-fMRI/)
- ![]()

- ![DataCamp: Reconstructing Brain MRI Images Using Deep Learning (Convolutional Autoencoder)](https://www.datacamp.com/tutorial/reconstructing-brain-images-deep-learning)

- ![]()
- ![]()
- ![DataCamp: Introduction to Deep Learning with Keras](https://www.datacamp.com/courses/introduction-to-deep-learning-with-keras)
- ![DataCamp: How to Learn Deep Learning in 2024 - A Complete Guide](https://www.datacamp.com/blog/how-to-learn-deep-learning)


## Books, Online Courses, Tutorials

Grisham, W., Abrams, M., Babiec, W. E., Fairhall, A. L., Kass, R. E., Wallisch, P., & Olivo, R. (2021). Teaching Computation in Neuroscience: Notes on the 2019 Society for Neuroscience Professional Development Workshop on Teaching. Journal of undergraduate neuroscience education : JUNE : a publication of FUN, Faculty for Undergraduate Neuroscience, 19(2), A185–A191. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8437361

## ChatGPT and Github Copilot{.smaller}

:::{.aside}

Gurdil, H., Soguksu, Y. B., Salihoglu, S., & Coskun, F. (2024). Integrating AI in Educational Measurement: ChatGPT's Efficacy in Item Response Theory Data Generation. arXiv preprint arXiv:2402.01731. https://doi.org/10.48550/arXiv.2402.01731



Kashefi, A., & Mukerji, T. (2023). ChatGPT for programming numerical methods. Journal of Machine Learning for Modeling and Computing, 4(2). https://doi.org/10.1615/JMachLearnModelComput.2023048492

Kim, D., Kim, T., Kim, Y., Byun, Y. H., & Yun, T. S. (2024). A ChatGPT-MATLAB framework for numerical modeling in geotechnical engineering applications. Computers and Geotechnics, 169, 106237. https://doi.org/10.1016/j.compgeo.2024.106237

Kosar, T., Ostojić, D., Liu, Y. D., & Mernik, M. (2024). Computer Science Education in ChatGPT Era: Experiences from an Experiment in a Programming Course for Novice Programmers. Mathematics, 12(5), 629. https://doi.org/10.3390/math12050629

Rahman, C. R., & Wong, L. (2023). How much can ChatGPT really help Computational Biologists in Programming?. arXiv preprint arXiv:2309.09126. https://doi.org/10.48550/arXiv.2309.09126

Sänger, M., De Mecquenem, N., Lewińska, K. E., Bountris, V., Lehmann, F., Leser, U., & Kosch, T. (2023). Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT. arXiv preprint arXiv:2311.01825. https://doi.org/10.48550/arXiv.2311.01825

Shen, Y., Ai, X., Soosai Raj, A. G., Leo John, R. J., & Syamkumar, M. (2024, March). Implications of ChatGPT for Data Science Education. In Proceedings of the 55th ACM Technical Symposium on Computer Science Education, 1, 1230-1236. https://doi.org/10.1145/3626252.3630874

Silva, C. A. G. D., Ramos, F. N., de Moraes, R. V., & Santos, E. L. D. (2024). ChatGPT: Challenges and Benefits in Software Programming for Higher Education. Sustainability, 16(3), 1245.
https://doi.org/10.3390/su16031245

Waseem, M., Das, T., Ahmad, A., Fehmideh, M., Liang, P., & Mikkonen, T. (2023). Using ChatGPT throughout the Software Development Life Cycle by Novice Developers. arXiv preprint.
https://doi.org/10.48550/arXiv.2310.13648

White, J., Hays, S., Fu, Q., Spencer-Smith, J., & Schmidt, D. C. (2023). Chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design. arXiv preprint. https://doi.org/10.48550/arXiv.2303.07839 

:::

## DataCamp, Coursera, Udemy, LinkedIn Learning

## ChatGPT Coding Prompts {.smaller}

:::{.columns}
::::{.column width="33%"}

**General coding workflows**  

- Code debugging 
- Code explanation 
- Code optimization 
- Code simplification 
- Code translation 
- Code quality and testing 
   - Compare function speeds
   - Write unit tests

::::
::::{.column width="33%"}

**Analysis workflows**  

- Python/R analysis 
   - Data generation/cleaning 
   - Data analysis workflow in pandas/tidyverse R
   - Data Aggregation
   - Data Merging

**Data visualization** 

- Data visualization in Python/R
   - Creating plots
   - Annotating/formatting 
   - Changing plot themes


::::
::::{.column width="33%"}

**Machine learning workflows**  

- General machine learning 
- Python/R machine learning 

  
**Time series analysis**  

- Python/R time series analysis 
  
**Natural language processing workflows**  
  
**Conceptual & career oriented prompts**

::::
:::


## Code Review & Collaboration

## Hackathons & Competitions

## Share & Teach


## Transformation & Manipulation in R {.smaller}

```{r}

Cosmetic_Surgery <- haven::read_sav("Cosmetic Surgery.sav")
head(Cosmetic_Surgery)


Cosmetic_Surgery <- Cosmetic_Surgery %>% 
  mutate(BDI_Centered = BDI - mean(BDI, na.rm = TRUE)) %>% 
  group_by(Clinic) %>% 
  mutate(BDI_Group_Centered = BDI - mean(BDI, na.rm = TRUE))

head(Cosmetic_Surgery)

```



