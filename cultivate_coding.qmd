---
title: "Learn to Code and </br>Excel in Your Scientific Endeavors"
subtitle: "Cultivate capabilities in neuro-data analytics and</br> high-performance statistical computing"
author: "Ryan Mears"
format: 
  revealjs:
    slide-number: true
    footnotes-hover: true
editor: visual
---

```{r}

library(tidyverse)

```

# Why Code?

::: r-fit-text
> Your closest\
> collaborator is\
> you six months ago\
> but you don't reply to email.
:::

-   

    ### **Karl Broman**

## Reasons to Learn to Code

-   Adaptability

-   Flexibility

-   Integration

-   Repeatability

-   Reusability

-   Sharability

## Code, How?

::: incremental
-   *Adapt* example code directly from online documentation
-   *Flexibly* accommodate new datatypes & formats
-   *Integrate* extensive additional code libraries & solutions from others
-   *Repeat* analysis with additional data or with changes to initial steps for reanalyzing the same data
-   *Reuse* with minor modifications for similar data
-   *Share* with others so they can modify, repeat, reuse, and build on code
:::


## Return on Investment for Code {.smaller}

| Advantages                      | Limitations                         |
|---------------------------------|-------------------------------------|
| Graphical Resource Independence | Explicit Command Requirement        |
| High Customization              | Reduced Usability for Novices       |
| High Composability              | Increased User Knowledge Dependence |
| Integration & Modularity        |                                     |
| Utility of Scale                |                                     |
| Searchability                   |                                     |


::: notes
1.  GUI is more accessible to novices (provides hints and guidance to users), but CLI requires explicit commands and user knowledge.
2.  
:::

## Code Libraries & APIs {.smaller}

-   **Code Libraries**: collections of pre-written code that can be used to perform tasks. Well designed code is reusable, modular, and efficient, and APIs are the interfaces that allow code libraries to be used.
-   **APIs** (Application Programming Interfaces): rules & protocols allowing users or other applications to interact with a software application.

::: columns
::: {.column width="50%"}
#### GUI (Graphical User Interface)

-   Intuitiveness and Accessibility
-   Limited to Pre-defined Options (i.e., graphical elements)
-   API (if it's not hidden) is constrained by graphical elements and rigidly structured
-   Event-Driven Programming (i.e., often a complex sequence and/or combination of user actions trigger code execution - clicks, drags, etc.)
:::

::: {.column width="\"50%"}
#### CLI (Command Line Interface)

-   Steep Learning Curve (..but intuition/accessibility improving)
-   High Flexibility and Control
-   Scripting, Customization, and Automation (APIs can be directly called in scripts)
-   Efficiency for Experienced Users (i.e., while GUI is linearly scaled, improvements in productivity can scale exponentially with CLI)
:::
:::

::: notes
Ways in which users or computers interact with APIs can vary significantly between GUIs and CLIs, reflecting differences in usability, flexibility, and the type of user experience they are designed to offer.

GUI

1.  **Intuitiveness and Accessibility:** GUIs are designed to be intuitive and accessible to users, offering graphical elements such as buttons, icons, and menus to interact with. The API in the context of a GUI might be more about how these elements are programmed to interact with the software's logic and data. Users don't usually interact with the API directly but through these graphical elements.

2.  **Limited to Pre-defined Options:** GUIs often limit the user to predefined operations or workflows designed by the application developers. The API interactions are therefore constrained by what the GUI offers, making it potentially less flexible but simpler for non-technical users.

3.  **Event-Driven Programming:** GUI APIs are heavily reliant on event-driven programming, where code execution is triggered by user actions like clicks, drags, and drops. This model affects how APIs are structured and utilized in GUI applications, focusing on responsiveness and user interaction feedback.

CLI

1.  **Flexibility and Control:** CLI provides a more direct and text-based way to interact with software, offering greater flexibility and control to the user. Users can combine commands in scripts, automate tasks, and utilize the full breadth of the API's capabilities. This makes CLIs particularly favored by developers and system administrators.

2.  **Scripting and Automation:** CLI allows for scripting and automation. Users can write scripts that directly call an application's API functions, enabling complex workflows and batch processing. This aspect is less inherent in GUIs, which are more designed for interactive use.

3.  **Steep Learning Curve:** The increased power and flexibility of CLIs come with a steeper learning curve. Users must be familiar with the command syntax and the software's API to effectively use it. This makes CLIs less accessible to non-technical users compared to GUIs.

4.  **Efficiency for Experienced Users:** For users who are comfortable with command-line interfaces, CLIs can offer a more efficient way to interact with software. Operations that might require several clicks in a GUI can often be performed with a single command in a CLI.
:::

## 

![](https://imgs.xkcd.com/comics/python.png){.absolute height=100% top=0% right=10%} 

XKCD^[<https://xkcd.com/353/>]


## Data Science Activities {.smaller}

::: columns
::: {.column width="50%"}
1.  Data Gathering, Preparation, and Exploration
2.  Data Representation and Transformation
3.  Computing with Data
4.  Data Modeling
5.  Data Visualization and Presentation
6.  Science about Data Science

*From Donoho, 2017.*[^2]
:::

::: {.column width="50%"}


:::
:::

[^2]: Donoho, D. (2017). 50 years of data science. Journal of</br> Computational and Graphical Statistics, 26(4), 745-766.</br> <https://doi.org/10.1080/10618600.2017.1384734>

## Statistics & Data Science Curricula {.smaller}

::: columns
::: {.column width="30%"}
**Programming**

-   Structured
-   Efficiency
-   HPC

**Data Formats**

-   Ragged arrays
-   Text data
-   Data cleaning
:::

::: {.column width="30%"}
**Data Tech**

-   RDBMS (SQL)
-   RegEx
-   XML
-   Shell commands
-   Web scraping
:::

::: {.column width="30%"}
**Work Flow**

-   Reproducibility
-   Web publishing
-   Revision control

**Statistical**

-   Simulations
-   Modern methods
-   Visualization

*From Hardin et al.,2015,*[^3]
:::
:::

[^3]: Hardin, J., Hoerl, R., Horton, N. J., Nolan, D., Baumer, B., Hall-Holt, O., ... & Ward, M. (2015). Data science in statistics curricula: Preparing students to “think with data”. The American Statistician, 69(4), 343-353. <http://dx.doi.org/10.1080/00031305.2015.1077729>

## Conventional Data Analysis Workflows

::: columns
::: {.column width="25%"}
::: {style="display: flex;"}
<div>

**SPSS & MS Office**

</br>

**Scripting in** </br> Matlab/Python

</div>
:::
:::

::: {.column width="70%"}
```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=2}

%%{init: {'theme':'forest'}}%%

graph LR
    A[Raw Data] -->|Import </br> </br> </tab> | B(Excel)
    B -->|Process and </tab> </br> </br> </tab> Analyze| C(SPSS)
    C -->|Present| D(PowerPoint)
    C -->|Document| E(Word)
    

    classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
    classDef orange fill:#f96,stroke:#333,stroke-width:1px;
    classDef white fill:#fff,stroke:#333,stroke-width:1px; 
    classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
    classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
    classDef purple fill:#6A2A60,stroke:#FFF,stroke-width:1.5px;
    classDef royalblue fill:#0021A5,stroke:#FFF,stroke-width:1.5px;
    
    class B,C green
    class A red
    class E royalblue
    class D purple

    
```

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=2}

%%{init: {'theme':'forest'}}%%

graph LR

    A[Raw Data] -->|Import </br> </br> </br> | B(IDE)
    B -->|Process </br> </br> </tab> | C(IDE)
    C -->|Analyze| B
    C -->|Present| D(PowerPoint)
    C -->|Document| E(Word)
    

    classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
    classDef orange fill:#f96,stroke:#333,stroke-width:1px;
    classDef white fill:#fff,stroke:#333,stroke-width:1px; 
    classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
    classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
    classDef purple fill:#6A2A60,stroke:#FFF,stroke-width:1.5px;
    classDef royalblue fill:#0021A5,stroke:#FFF,stroke-width:1.5px;
    
    class B,C green
    class A red
    class E royalblue
    class D purple
    
    
```
:::
:::

# Open Science

Promotion of Scientific Transparency & Reliability\
Presents Advantages for Research Training

## Open Science {.smaller}

![](RR-Open2021.jpg){width="75%"}[^4]

[^4]: Garrett-Ruffin, S., Hindash, A. C., Kaczkurkin, A. N., Mears, R. P., Morales, S., Paul, K., ... & Keil, A. (2021). Open science in psychophysiology: An overview of challenges and emerging solutions. International Journal of Psychophysiology, 162, 69-78.<https://doi.org/10.1016/j.ijpsycho.2021.02.005>

::: notes
Fig. 1. Open science practices affect the research process at multiple levels.

The process of experimental research, involving steps ranging from hypothesis generation to drawing conclusions, is positively affected by various open science practices such as pre-registration and multi-laboratory studies. Direct replication requires sequential repetition of measurements and treatments. In a multisite study, identical measurements and treatments are carried out simultaneously between multiple similar experiment settings. Replicability in a multisite study thus supports the robustness of study outcomes. In this context, Computational Reproducibility addresses \*Researcher Degrees of Freedom by constraining the influences of user defined parameters, code, and computing environment on analysis outcome. Likewise, preregistration precludes questionable research practices such as HARKing (hypothesizing after the results are known) by eliminating the possibility of outcome-dependent decision making.
:::


## Computational Reproducibility {.smaller}

::: columns
::: {.column width="60%"}
::: {style="display: flex;"}
<div>

</br>

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=1.5, fig.height=12}

graph TD;
      A[coarse] ----->|Specificity </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab>   | B[granular]


```

</div>

::: {.middle style="text-align: center"}
```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=9, fig.height=2}


graph LR;
    A[Abstract] ---> |Representativeness </br> </br> </tab> | B[Concrete]


```

|                                                    |                                                                                                     |     |
|-------------------------|--------------------------|---------------------|
| [**Computing Environment**]{style="color: black;"} | [**Workflow**/</br>**Pipeline**]{style="color: black;"}                                             |     |
| [**Raw Data** </br> & Code]{style="color: black;"} | [**Analysis Derivatives** </br> - *derived variables* </br> - *stats/plots*]{style="color: black;"} |     |

: {.table .table-bordered .text-white .border .border-white .rounded-3 "}

::: {.callout-important title="Essential Components for Reproducibility"}

Misalignments between above components  
  
leads to irreproducible results.

:::

:::
:::
:::

::: {.column width="40%"}

Essential components for analysis reproducibility include:

-   **Computing Environment**: The software and hardware environment in which the analysis is conducted
-  **Workflow/Pipeline**: The sequence of steps involved in the analysis
-   **Raw Data & Code**: The data and code used in the analysis
-   **Analysis Derivatives**: The results of the analysis, such as derived variables, statistics, and plots

:::
:::


## Computational Reproducibility {.smaller}

::: columns
::: {.column width="60%"}
::: {style="display: flex;"}
<div>

</br>

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=1.5, fig.height=12}

graph TD;
      A[coarse] ----->|Specificity </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab>   | B[granular]


```

</div>

::: {.middle style="text-align: center"}
```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=9, fig.height=2}


graph LR;
    A[Abstract] ---> |Representativeness </br> </br> </tab> | B[Concrete]


```

|                                                   |                                                                                                    |     |
|------------------------|--------------------------|----------------------|
| [**Computing Environment**]{style="color: red;"}  | [**Workflow**/</br>**Pipeline**]{style="color: gray;"}                                             |     |
| [**Raw Data** </br> & Code]{style="color: gray;"} | [**Analysis Derivatives** </br> - *derived variables* </br> - *stats/plots*]{style="color: gray;"} |     |

: {.table .table-bordered .text-white .border .border-white .rounded-3 "}
:::
:::
:::

::: {.column width="40%"}
</br> </br>

Learn how high performance computing (HPC) works: 

-   [ ] Shell scripting for generalizing tasks across hardware/operating systems
-   [ ] Filesystems
-   [ ] Computing abstractions
-   [ ] Operating systems 
-   [ ] Graphics hardware 
-   [ ] Applications/software


:::
:::

## Computational Reproducibility {.smaller}

::: columns
::: {.column width="60%"}
::: {style="display: flex;"}
<div>

</br>

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=1.5, fig.height=12}

graph TD;
      A[coarse] ----->|Specificity </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab>   | B[granular]


```

</div>

</br>

::: {.middle style="text-align: center"}
```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=9, fig.height=2}


graph LR;
    A[Abstract] ---> |Representativeness </br> </br> </tab> | B[Concrete]


```

|                                                   |                                                                                                    |     |
|------------------------|--------------------------|----------------------|
| [**Computing Environment**]{style="color:gray;"}  | [**Workflow**/</br>**Pipeline**]{style="color:  red;"}                                             |     |
| [**Raw Data** </br> & Code]{style="color: gray;"} | [**Analysis Derivatives** </br> - *derived variables* </br> - *stats/plots*]{style="color: gray;"} |     |

: {.table .table-bordered .text-white .dark .border .border-white .rounded-3"}
:::
:::
:::

::: {.column width="40%"}
</br> </br>

Learn how to design a workflow/pipeline:

- [ ] Monitor 
- [ ] Automate 
- [ ] Debug 
- [ ] Document
- [ ] Version control
- [ ] Share
- [ ] Test
- [ ] Containerizate
- [ ] Scale




:::
:::

## Computational Reproducibility {.smaller}

::: columns
::: {.column width="60%"}
::: {style="display: flex;"}
<div>

</br>

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=1.5, fig.height=12}

graph TD

      A[coarse] ----->|Specificity </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab>   | B[granular]


```

</div>

::: {.middle style="text-align: center"}
```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=9, fig.height=2}


graph LR;
    A[Abstract] ---> |Representativeness </br> </br> </tab> | B[Concrete]


```

|                                                   |                                                                                                    |     |
|------------------------|--------------------------|----------------------|
| [**Computing Environment**]{style="color: gray;"} | [**Workflow**/</br>**Pipeline**]{style="color: gray;"}                                             |     |
| [**Raw Data** </br> & Code]{style="color: red;"}  | [**Analysis Derivatives** </br> - *derived variables* </br> - *stats/plots*]{style="color: gray;"} |     |

: {.table .table-bordered .text-white .border .border-white .rounded-3 }
:::
:::
:::

::: {.column width="40%"}
</br> </br>

-   [ ] learn how to use this
-   [ ] dataframe concepts
-   [ ] data wrangling
-   [ ] data visualization
-   [ ] statistical modeling
:::
:::

## Computational Reproducibility {.smaller}

::: columns
::: {.column width="60%"}
::: {style="display: flex;"}
<div>

</br>

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=1.5, fig.height=12}
graph TD

      A[coarse] ----->|Specificity </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab> </tab>   | B[granular]


```

</div>

::: {.middle style="text-align: center"}
```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=9, fig.height=2}


graph LR;
    A[Abstract] ---> |Representativeness </br> </br> </tab> | B[Concrete]


```

|                                                   |                                                                                                  |     |
|-------------------------|--------------------------|---------------------|
| [**Computing Environment**]{style="color: gray;"} | [**Workflow**/</br>**Pipeline**]{style="color: gray;"}                                           |     |
| [**Raw Data** </br> & Code]{style="color: gray;"} | [**Analysis Derivatives** </br> - *derived variables* </br> -*stats/plots*]{style="color: red;"} |     |

: {.table .table-bordered .text-white .border .border-white .rounded-3 }
:::
:::
:::

::: {.column width="40%"}
</br> </br>

-   [ ] Store results of data wrangling in Python/R
-   [ ] Learn how to save graphs in Python/R
-   [ ] Record/save and communicate metadata info 
-   [ ] Record/save new variables/data structures created in intermediate pipeline steps

:::
:::

# Workflows (Pipelines) 

Sequence of Data Processing Steps  
  
Processing Algorithms  
  
Encoding Steps & Format Manipulations  
  
Data Validation Conditions  
  
User-Selected Data/Signal Optimizations  
  
Raw, Intermediate, Final Data Files  
   



## Types of Workflows {.smaller}

::: columns
::: {.column width="25%"}
::: {style="display: flex;"}
<div>

</br>

**SPSS & MS Office**

</br></br>

**Scripting in** </br> Matlab / Python

</br>

**Hybrid Interactive Python / Matlab**

</br></br>

**R & RStudio Workflow**

</div>
:::
:::

::: {.column width="70%"}
```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=2}

%%{init: {'theme':'forest'}}%%

graph LR
    A[Raw Data] -->|Import </br> </br> </tab> | B(Excel)
    B -->|Process and </tab> </br> </br> </tab> Analyze| C(SPSS)
    C -->|Present| D(PowerPoint)
    C -->|Document| E(Word)
    

    classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
    classDef orange fill:#f96,stroke:#333,stroke-width:1px;
    classDef white fill:#fff,stroke:#333,stroke-width:1px; 
    classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
    classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
    classDef purple fill:#6A2A60,stroke:#FFF,stroke-width:1.5px;
    classDef royalblue fill:#0021A5,stroke:#FFF,stroke-width:1.5px;
    
    class B,C green
    class A red
    class E royalblue
    class D purple

    
```

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=2}

%%{init: {'theme':'forest'}}%%

graph LR

    A[Raw Data] -->|Import </br> </br> </br> | B(IDE)
    B -->|Process </br> </br> </tab> | C(IDE)
    C -->|Analyze| B
    C -->|Present| D(PowerPoint)
    C -->|Document| E(Word)
    

    classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
    classDef orange fill:#f96,stroke:#333,stroke-width:1px;
    classDef white fill:#fff,stroke:#333,stroke-width:1px; 
    classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
    classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
    classDef purple fill:#6A2A60,stroke:#FFF,stroke-width:1.5px;
    classDef royalblue fill:#0021A5,stroke:#FFF,stroke-width:1.5px;
    
    class B,C green
    class A red
    class E royalblue
    class D purple
    
    
```

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=2}

%%{init: {'theme':'forest'}}%%

graph LR
    A[Raw Data] -->|Import </br> </br> </tab> | B(Jupyter Notebook)
    B -->|Process and </tab> </br> </br> </tab> Analyze| C(Jupyter Notebook)
    C -->|Present| D(PowerPoint)
    C -->|Document| E(Word)
    

    classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
    classDef orange fill:#f96,stroke:#333,stroke-width:1px;
    classDef white fill:#fff,stroke:#333,stroke-width:1px; 
    classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
    classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
    classDef purple fill:#6A2A60,stroke:#FFF,stroke-width:1.5px;
    classDef royalblue fill:#0021A5,stroke:#FFF,stroke-width:1.5px;
    
    class B,C green
    class A red
    class E royalblue
    class D purple
    
    
```

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=2}

    
%%{init: {'theme':'forest'}}%%

graph LR

      subgraph RStudio
      
            B[RStudio]-->|Process</tab> </br> </br> </tab>| C[RStudio]
            C -->| Visualize</tab> </br> </br> </tab> | D[RStudio]
            D -->| Analyze</tab> </br> </br> </tab> | B

        end

      subgraph Output
            D --> F[Documents /</br>Slides /</br>Dashboards /</br>Websites]
        end
        
      subgraph Input
            A[Raw Data] -->|Import </br> </br> </tab> | B
        end
 
    
    classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
    classDef orange fill:#f96,stroke:#333,stroke-width:1px;
    classDef white fill:#fff,stroke:#333,stroke-width:1px; 
    classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
    classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
    classDef purple fill:#6A2A60,stroke:#FFF,stroke-width:1.5px;
    classDef royalblue fill:#0021A5,stroke:#FFF,stroke-width:1.5px;
    
    class RStudio,Input,Output white
    class B,C green
    class A red
    class D royalblue
    class F purple

    
```
:::
:::



## Video Behavior Analysis Workflow {.smaller}

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=3}

%%{init: {'theme':'forest'}}%%

graph LR

    subgraph Analysis
       s6 -->  s7{Inferential Stats}
    end
 
    subgraph ETHZ-INS/DLCAnalyzer
        s3 --> s4[Data Wrangling </br> Graphing]
        s5 --> s6[Data Wrangling]
        s6 --> s4
        
        subgraph dlca[</tab> </br> Derivative Variables / Data Visualizations]
          s4 --> s5[unsupervised</br> classification</br> and clustering]
        end
    end

    subgraph DeepLabCut

        subgraph Training
            s1[add keypoints]-->s2[run training]
            s2 --> s2a[analyze</br> training]
            s2a -->s2

        end

        subgraph Decoding
            s2a --> s3a[add</br> videos] 
            
        end
        
         
        s3a --> s3[import data</br>transform]
       
    end
    
    subgraph Read-in-Data[Raw Data]
        s0[import video] --> s1
    end
    
 %% Notice that no text in shape are added here instead that is appended further down
    

    %% Comments after double percent signs

     classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
     classDef orange fill:#f96,stroke:#333,stroke-width:1px;
     classDef white fill:#fff,stroke:#333,stroke-width:1px;
     classDef sq stroke:#f66,stroke-width:1px;
     classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
     classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
     class sq,Analysis green
     class ETHZ-INS/DLCAnalyzer orange
     class Training,dlca,s0 white
     class DeepLabCut white
     class Decoding blue
     class Read-in-Data red
     
```


::: columns
::: {.column width="50%"}

![](DLC_youtube.png){width=50%}
[on Github](https://github.com/DeepLabCut/DeepLabCut)/
[Workshop](https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials/blob/main/DLCcourse.md>)



:::

::: {.column width="50%"}



- [Data from DLC (Python):wrangle/plot (R) ](https://github.com/ETHZ-INS/DLCAnalyzer)

- Unsupervised Clustering: [(R)](https://github.com/ETHZ-INS/BehaviorFlow)
/ [(R Shiny)](https://github.com/ETHZ-INS/pupillometry)



:::
:::

:::{.aside}

Mathis, A., Schneider, S., Lauer, J., & Mathis, M. W. (2020). A primer on motion capture with deep learning: principles, pitfalls, and perspectives. Neuron, 108(1), 44-65. <https://doi.org/10.1016/j.neuron.2020.09.017>

Sturman, O, von Ziegler, L, Schläppi, C et al.(2020). Deep learning-based behavioral analysis reaches human accuracy and is capable of outperforming commercial solutions. Neuropsychopharm. 45, 1942–52  <https://doi.org/10.1038/s41386-020-0776-y>

:::



## Video Behavior Analysis Workflow {.smaller}

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=3}

%%{init: {'theme':'forest'}}%%

graph LR

    subgraph Analysis
       s6 -->  s7{Inferential Stats}
    end
 
    subgraph ETHZ-INS/DLCAnalyzer
        s3 --> s4[Data Wrangling </br> Graphing]
        s5 --> s6[Data Wrangling]
        s6 --> s4
        
        subgraph dlca[</tab> </br> Derivative Variables / Data Visualizations]
          s4 --> s5[unsupervised</br> classification</br> and clustering]
        end
    end

    subgraph DeepLabCut

        subgraph Training
            s1[add keypoints]-->s2[run training]
            s2 --> s2a[analyze</br> training]
            s2a -->s2

        end

        subgraph Decoding
            s2a --> s3a[add</br> videos] 
            
        end
        
         
        s3a --> s3[import data</br>transform]
       
    end
    
    subgraph Read-in-Data[Raw Data]
        s0[import video] --> s1
    end
    
 %% Notice that no text in shape are added here instead that is appended further down
    

    %% Comments after double percent signs

     classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
     classDef orange fill:#f96,stroke:#333,stroke-width:1px;
     classDef white fill:#fff,stroke:#333,stroke-width:1px;
     classDef sq stroke:#f66,stroke-width:1px;
     classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
     classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
     class sq,Analysis green
     class ETHZ-INS/DLCAnalyzer orange
     class Training,dlca,s0 white
     class DeepLabCut white
     class Decoding blue
     class Read-in-Data red
     
```


::: columns
::: {.column width="25%"}

![](covnet_dlc.jpeg){width=80%}

:::

::: {.column width="25%"}

![](mpc21_ket_wag_10s.gif)

:::

::: {.column width="25%"}

![](mpc20_ket_wag_10s.gif)

:::

::: {.column width="25%"}

![](kuiper_rayleigh_thresh_div_1to9_hstg_mpc21.jpg)

:::

:::

:::{.aside}

Mathis, A., Schneider, S., Lauer, J., & Mathis, M. W. (2020). A primer on motion capture with deep learning: principles, pitfalls, and perspectives. Neuron, 108(1), 44-65. <https://doi.org/10.1016/j.neuron.2020.09.017>

Sturman, O, von Ziegler, L, Schläppi, C et al.(2020). Deep learning-based behavioral analysis reaches human accuracy and is capable of outperforming commercial solutions. Neuropsychopharm. 45, 1942–52  <https://doi.org/10.1038/s41386-020-0776-y>

:::


:::{.notes}

Figure 4. Schematic overview of possible design choices for model architectures and training process (A) A simple, but powerful variant (18) is a
ResNet-50 (54) architecture adapted to replace the final down-sampling
operations by atrous convolutions (60) to keep a stride of 16, and then
a single deconvolution layer to upsample to output maps with stride 8. It
also forms the basis of other architectures (e.g. 28). The encoder can
also be exchanged for different backbones to improve speed or accuracy
(see Box 2). (B) Other approaches like stacked hourglass networks (26),
are not pre-trained and employ skip connections between encoder and
decoder layers to aid the up-sampling process. (C) For training the network, the training data comprising input images and target heatmaps
is used. The target heatmap is compared with the forward prediction.
Thereby, the parameters of the network are optimized to minimize the
loss that measures the difference between the predicted heatmap and
the target heatmap (ground truth).

When the learning problem is instead cast
as identifying the keypoint locations on a grid of pixels, the
output resolution needs to be increased first, often by deconvolutional layers (18, 28). We denote this part of the network
as the decoder, which takes downsampled features, possibly
from multiple layers in the encoder hierarchy, and gradually upsamples them again to arrive at the desired resolution.
The first models of this class were Fully Convolutional Networks (69), and later DeepLab (60). Many popular architectures today follow similar principles. Design choices include the use of skip connections between decoder layers, but
also regarding skip connections between the encoder and decoder layers. Example encoder–decoder setups are illustrated
in Figure 4. The aforementioned building blocks—encoders
and decoders—can be used to form a variety of different approaches, which can be trained end-to-end directly on the target task (i.e., pose estimation).

:::


## MNE-BIDS-Pipeline {.smaller}

```{mermaid, echo=FALSE, warning=FALSE, message=FALSE}

%%{init: {'theme':'forest'}}%%

graph LR;
        subgraph Sensor-level-analysis

       A[Decode time-by-time </br>  using a 'sliding' estimator] --> B[Time-frequency </br> decomposition] 
       B --> C[Decoding </br> based on </br> common spatial patterns] 
       C --> D[Noise covariance </br> estimation] 
       D --> E[Group average </br> at the </br> sensor level] 
   end
  
    subgraph Preprocessing

       l1[Assess </br> channel-wise </br> data quality] --> l2[Estimate </br> head positions] 
       l2 --> l3[Apply low- and </br> high-pass filters] 
       l3 --> l4[Temporal regression </br> for artifact removal]
       l4 --> l5a[Fit ICA] 
       l5a --> l6a[Find ICA </br> artifacts] 
       l4 --> l5b[Extract epochs] 
       l5b --> l7[Apply ICA] 
       l6a --> l7 --> l8[Remove epochs </br> based on  </br> PTP amplitudes]  
       l5b --> l6b[Apply SSP] 
       l4 --> l5c[Compute SSP] 
       l5c --> l6b[Apply SSP] 
       l6b --> l8[Remove epochs </br> based on </br> PTP amplitudes] 
       l8 --> l9[Extract </br> evoked data for </br> each condition] 
       l9 --> l10[Decode pairs of </br>  conditions based on </br> entire epochs] 
    end
    

   
   %%l0[Raw Data] --> l1
   %%E --> F[Output </br> Statistical </br> Analysis Pipeline]    
   
   classDef green fill:#9f6,stroke:#333,stroke-width:0.5px;
   classDef orange fill:#f96,stroke:#333,stroke-width:1px;
   classDef white fill:#fff,stroke:#333,stroke-width:1px;
   classDef sq stroke:#f66,stroke-width:1px;
   classDef blue fill:#6699cc,stroke:#333,stroke-width:1px;
   classDef red fill:#D32737,stroke:#FFF,stroke-width:1.5px;
     
   class Preprocessing,Sensor-level-analysis white

```

::: columns
::: {.column width="50%"}

<https://github.com/mne-tools>

::: 
::: {.column width="50%"}

<https://mne.tools/stable/index.html>


:::
:::






::: {background-opacity="0.1" background-iframe="https://mne.tools/stable/auto_tutorials/index.html#introductory-tutorials" style="text-align: left; margin-top: 1em"}
## [MNE-Python: Introductory Tutorials](https://mne.tools/stable/auto_tutorials/index.html#introductory-tutorials){preview-link="true" style="text-align: center"}
:::



## MNE-BIDS-pipeline {.smaller}

#### Prepare your dataset

MNE-BIDS-Pipeline only works with BIDS-formatted raw data.

![](https://mne.tools/mne-bids/assets/MNE-BIDS.png)

## MNE-BIDS-pipeline {.smaller}

#### Prepare your dataset

MNE-BIDS-Pipeline only works with BIDS-formatted raw data.

#### Create a configuration file

All parameters of the pipeline are controlled via a configuration file. Create a template:

`mne_bids_pipeline --create-config=/path/to/custom_config.py`

```{python, eval=FALSE, echo=TRUE}

import numpy as np

study_name = "ds000247"
bids_root = f"~/mne_data/{study_name}"
deriv_root = f"~/mne_data/derivatives/mne-bids-pipeline/{study_name}"

subjects = ["0002"]
sessions = ["01"]
task = "rest"
task_is_rest = True

crop_runs = (0, 100)  # to speed up computations

ch_types = ["meg"]
spatial_filter = "ssp"

l_freq = 1.0
h_freq = 40.0

rest_epochs_duration = 10
rest_epochs_overlap = 0

epochs_tmin = 0
baseline = None

```

-   [from example ds000247](https://mne.tools/mne-bids-pipeline/stable/examples/ds000247.html#configuration)

## MNE-BIDS-pipeline {.smaller}

#### Prepare your dataset

#### Create a configuration file

#### Run the pipeline

A config file controls main pipeline parameters. CLI runs all (or part with an override).

-   Re-running a specific stage of the pipeline for additional data
    -   `mne_bids_pipeline --config=custom_config.py --steps=preprocessing --subjects=0051,0052,0053`
-   Running the pipeline with different parameters for a specific stage (e.g., changing filter cutoffs, interpolating bad channels, etc.)
    -   `mne_bids_pipeline --config=custom_config.py --steps=preprocessing/ica`
-   Running the pipeline with different parameters for a specific subject or session
    -   `mne_bids_pipeline --config=custom_config.py --steps=preprocessing/ica --session=cond --subjects=0051,0052,0053`

# Open Science: Finding and Sharing Analysis Code


## Why use Git?

-   Remote repo: accessed & <br> updated anytime
-   Solutions for continuous <br> merging of sets of changes

![](team-git.png){.absolute top="0" right="-10" height="450"}

::: aside
> Perez-Riverol, *et al.* (2016). Ten Simple Rules for Taking Advantage of Git & GitHub. *PLoS computational biology*, [12(7), e1004947.](https://doi.org/10.1371/journal.pcbi.1004947)
:::

## Key Feature of Git: Timeline Control

![](reachability-example.png)

[Example from *Think Like (a) Git: A Guide for the Perplexed*](https://think-like-a-git.net/sections/graph-theory/reachability.html)

::: notes
You can think of this graph as a set of three parallel universes with time flowing from left to right, so that A is the beginning of recorded history. (The arrow represents the "follows" or "is subsequent to" relationship, so you might say that "B follows A".)
:::

## Key Feature of Git: Timeline Control

![](reachability-exampleE.jpg)

## Key Feature of Git: Timeline Control

![](reachability-exampleK.jpg)

::: notes
-   If you start from E, the history you'll see is A, B, C, D, E.
:::

## Key Feature of Git: Timeline Control

![](reachability-exampleH.jpg)

::: notes
-   If you start from K, the history you'll see is A, B, C, I, J, K. But the really important thing about this is that no matter which node you start with, some **parts of the graph will be unreachable** to you.

*turn it around*: **Depending** on where you start, you can **reach parts** of the graph that you couldn't get to otherwise.
:::

## Key Feature of Git: Timeline Control

![](graph.png)

##  {background-image="Git-4yrs-up.png" background-size="cover"}

::: notes
[Git-4yrs-up 1h 31m](https://youtu.be/1ffBJ4sVUb4?t=5460)
:::

## 

![](https://imgs.xkcd.com/comics/git.png){.absolute top=10% right=40%}

 
 XKCD ^[<https://xkcd.com/1597/>]

::: aside

 by Randall Munroe

:::

## Free Software on GitHub for Psychology & Neuroscience {.smaller}

::: columns
::: {.column width="30%"}
**Python Frameworks**

-   [Anaconda](https://github.com/ContinuumIO)
-   [PyPi](https://pypi.org/)[**(Python Package Index)**]{style="font-size:75%"}
-   [Jupyter](https://github.com/jupyter/)/[Pyodide](https://github.com/pyodide/pyodide)
-   [Numpy](https://github.com/numpy/numpy) / [Scipy](https://github.com/scipy/scipy)
-   [Pandas](https://github.com/pandas-dev/pandas)/[Statsmodels](https://github.com/statsmodels/statsmodels)
-   [Matplotlib](https://github.com/matplotlib/matplotlib)/[Seaborn](https://github.com/seaborn)
-   [Keras](https://github.com/keras-team/keras)/[TensorFlow](https://github.com/tensorflow/tensorflow)
-   [PyTorch](https://github.com/pytorch/pytorch)
-   [Scikit-learn](https://github.com/scikit-learn/scikit-learn)
:::

::: {.column width="40%"}
**R**

-   [Tidyverse R](https://github.com/tidyverse)
-   [ROpenSci](https://github.com/ropensci)
-   CRAN Task Views </br> [**(Comprehensive R Archive Network)**]{style="font-size:75%"}
    -   [Psychometrics](CRAN%20Task%20View:%20Psychometric%20Models%20and%20Methods)
    -   [Bayesian Inference](https://cran.r-project.org/web/views/Bayesian.html)
    -   [Reproducible Research](https://cran.r-project.org/web/views/ReproducibleResearch.html)
    -   [Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html)
    -   [Teaching Statistics](https://cran.r-project.org/web/views/TeachingStatistics.html)
:::

::: {.column width="30%"}
**Psychology & Neuroscience**

Python

-   [MNE](https://github.com/mne-tools)
-   [PsychoPy](https://www.psychopy.org/)
-   [DeepLabCut](https://github.com/DeepLabCut/DeepLabCut)
-   [Movement](https://github.com/neuroinformatics-unit/movement)

R

-   [DLCAnalyzer](https://github.com/ETHZ-INS/DLCAnalyzer)
-   [qualtRics](https://github.com/ropensci/qualtRics)
:::
:::

::: notes
:::

# Data Science Skills Essential to Statistical Computation 

## Grammar of Graphics {.smaller}

::: columns
::: {.column width="25%"}
![](GofG_wilkinson.png)[^5]
:::

::: {.column width="65%"}
-   Leland Wilkinson coined "Grammar of Graphics".[^6]

-   Headed APA statistical reporting guideline taskforce.[^7]

-   Owner of SYSTAT and then became Chief Scientist at SPSS after Systat was sold to SPSS.

-   SPSS utilizes Grammar of Graphics, but mostly in SPSS syntax or largely hidden from the user at the GUI-level.
:::
:::

[^5]: Wilkinson, L. (1999). The Grammar of Graphics. Springer.

[^6]: Wilkinson, L. (2012). The Grammar of Graphics. In: Gentle, J., Härdle, W., Mori, Y. (eds) Handbook of Computational Statistics. Springer Handbooks of Computational Statistics. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-21551-3_13

[^7]: Wilkinson, L. (1999). Statistical methods in psychology journals: Guidelines & explanations. American Psychologist, 54, 594.

![](240px-SPSS_logo.svg.png){.absolute top="-5%" right="10" height="100"} ![](systat.png){.absolute top="-5%" right="15%" height="100"} ![](GofG_dataflow.png){.absolute top="11%" right="-2%" width="13%"} ![](GofG_aesth.png){.absolute bottom="20%" right="12%" width="55%"}

::: notes
Architecture Figure 13.1 Data Flow in the Grammar of Graphics. This figure shows a dataflow diagram that contains the seven GOG components. This dataflow is a chain that describes the sequence of mappings needed to produce a statistical graphic from a set of data. The first component (Variables) maps data to an object called a varset (a set of variables). The next three components (Algebra, Scales, Statistics) are transformations on varsets. The next component (Geometry) maps a varset to a graph and the next (Coordinates) embeds a graph in a coordinate space. The last component (Aesthetics) maps a graph to a visible or perceivable display called a graphic.

The dataflow architecture implies that the subtasks needed to produce a graphic from data must be done in this specified order. Imposing an order would appear to be unnecessarily restrictive, but changes of this ordering can produce meaningless graphics. For example, if we compute certain statistics on variables (e.g., sums) before scaling them (e.g., log scales), we can produce statistically questionable results because the log of a sum is not the sum of the logs.

The dataflow in Fig. 13.1 has many paths through it. We can choose different designs (factorial, nested, :::), scales (log, probability, :::), statistical methods (means, medians, modes, smoothers, : : :), geometric objects (points, lines, bars, : : :), coordinate systems (rectangular, polar, : : :), and aesthetics (size, shape, color, : : :). These paths reveal the richness of the system. The remainder of this article will summarize the seven GOG components, delineate these paths, and then briefly introduce sample applications.

The term aesthetics derives from a Greek word that means perception. The derivative modern meanings of beauty, taste, and artistic criteria arose in the eighteenth century. We have chosen the name aesthetics to describe the class of functions that turn theoretical graphs into perceivable graphics because of its original connotations and because the modern word perception is subjective rather than objective;
:::

## Grammar of Graphics {.smaller}

::: columns
::: {.column width="50%"}
[**Data**]{style="color: gray;"}: ingredients for plot\
[**Aesthetics**]{style="color: red;"}: *(aes)*, to make data visible

-   [*x*]{style="color: red;"}, [*y*]{style="color: red;"}: variable along x- & y-axis
-   [*colour*]{style="color: red;"} & [*fill*]{style="color: red;"}: geoms color by data
-   [*group*]{style="color: red;"}: what group a geom belongs to
-   [*shape*]{style="color: red;"}: object-type for a plot point
-   [*linetype*]{style="color: red;"},[*size*]{style="color: red;"}: size scaling
-   [*alpha*]{style="color: red;"}: geom transparency

[**Geometric objects**]{style="color: blue;"}: `geom_`

-   [*geom_point()*]{style="color: blue;"}: scatterplot
-   [*geom_line()*]{style="color: blue;"} & [geom_path()\*]{style="color: blue;"}: lines
-   [*geom_boxplot()*]{style="color: blue;"}: box & whiskers plot
:::

::: {.column width="50%"}
::: center
![](gglayers.png)

-   [*geom_bar()*]{style="color: blue;"}: categorical x
-   [*geom_histogram()*]{style="color: blue;"}: continuous x
-   [*geom_violin()*]{style="color: blue;"}: distribution
-   [*geom_smooth()*]{style="color: blue;"}: data-based line fit
:::
:::
:::

## Grammar of Graphics {.smaller}

::: columns
::: {.column width="50%"}
![](layers_plots.png)
:::

::: {.column width="50%"}
![](gglayers.png)

[**Facets**]{style="color: orange;"}: [*facet_wrap()*]{style="color: orange;"} or [*facet_grid()*]{style="color: orange;"}

[**Statistics**]{style="color: green;"}: [stat\_\*()]{style="color: green; font-weight: bold;"}: similar to geoms [**Coordinates**]{style="color: darkkhaki;"}: fitting data onto page [*coord_cartesian()*]{style="color: darkkhaki;"} [*coord_polar()*]{style="color: darkkhaki;"} [*coord_flip()*]{style="color: darkkhaki;"}

[**Themes**]{style="color: purple;"}: overall visual defaults

-   fonts, colors, shapes, outlines

:::
:::

::: aside
Create a simple plot object:

`plot.obj <- ggplot()`

Add geometric layers:

`plot.obj <- plot.object + geom_*()`

Add appearance layers:

`plot.obj <- plot.obj + coord_*() + theme()`

Repeat step 2-3 until satisfied, then print:

`plot.obj or print(plot.obj)`
:::

::: notes
https://r.qcbs.ca/workshop03/book-en/grammar-of-graphics-gg-basics.html

GG refers to the Grammar of Graphics, a system for describing and building graphs. As in grammar of a language let's use a sentence. A sentence is a combination of words that form a complete thought. Usually, a sentence has a verb and a subject, but you can also layer on adjectives, adverbs, and other parts of speech to make the sentence more complex. With each addition you can change and modify meaning.

By analogy, in the same way that grammar is a system for describing a language, the Grammar of Graphics is a system for describing graphs. The ggplot2 package is a popular implementation of the Grammar of Graphics in R.
:::

## 

![](https://imgs.xkcd.com/comics/change_in_slope.png){.absolute top=15% right=10%}
  
XKCD ^[<https://xkcd.com/2701/>]

::: aside

 by Randall Munroe

:::

## Data Transformation & Manipulation {.smaller}

## SPSS Transformation & Manipulation {.smaller}

::: {.fragment .aside}
</br> </tab> </br> 1- Select "variables-to-cases" </br> 2- Select "one" var to restructure </br> 3- Select ID var, 4 vars to transpose, & 1 var to not change </br> 4- Select "one" var to create
:::

::: r-stack
![](spss_restr1-2.png){.fragment .absolute top="50" left="0" width="90%"} ![](spss_restr3-4.png){.fragment .absolute top="100" left="40" width="90%"}
:::

::: notes
-   Cleophas, T. J., Zwinderman, A. H., Cleophas, T. J., & Zwinderman, A. H. (2020). Restructure Data Wizard for Data Classified the Wrong Way (20 Patients). *Machine Learning in Medicine–A Complete Overview*, 117-121.

-   Ravand, H. (2019). Item response theory using hierarchical generalized linear models. *Practical Assessment, Research, and Evaluation, 20(1)*, 7.

-   Field, A. (2013). Chapter 20 Multilevel linear models. Discovering statistics using IBM SPSS statistics: Sage Publications Ltd.
:::

## SPSS Transformation & Manipulation {.smaller}

::: {.fragment .aside}
</br> </tab> </br> 5- Select "Sequential Numbers" and name index var as "Time", </br> 6- Select "Options" </br> 7- Select "restructure and/or syntax" and "Finish"
:::

![](spss_restr1-2.png){.absolute top="50" left="0" width="90%"} ![](spss_restr3-4.png){.absolute top="100" left="40" width="90%"}

::: r-stack
![](spss_restr5-6.png){.fragment .absolute top="150" left="80" width="90%"}

![](spss_restr7.png){.fragment .absolute top="200" right="-20" width="45%"}
:::

::: notes
-   Cleophas, T. J., Zwinderman, A. H., Cleophas, T. J., & Zwinderman, A. H. (2020). Restructure Data Wizard for Data Classified the Wrong Way (20 Patients). *Machine Learning in Medicine–A Complete Overview*, 117-121.

-   Ravand, H. (2019). Item response theory using hierarchical generalized linear models. *Practical Assessment, Research, and Evaluation, 20(1)*, 7.

-   Field, A. (2013). Chapter 20 Multilevel linear models. Discovering statistics using IBM SPSS statistics: Sage Publications Ltd.
:::

## Transformation & Manipulation in R {.smaller}

```{r}
library(tidyverse)
library(haven)

honeymoon_period <- haven::read_sav("honeymoon_period.sav")

honeymoon_period2 <- head(honeymoon_period,5)
knitr::kable(honeymoon_period2)

# knitr::kable(honeymoon_period2, col.names = gsub("atisfaction", "",names(honeymoon_period2)))

```

```{r, echo=TRUE}
#| code-line-numbers: "2|3-4"
 
honeymoon_period_longer <- honeymoon_period %>% 
  pivot_longer(cols = starts_with("Satisfaction_"), 
               names_to = "Life_satisfaction", 
               values_to = "Time")
```

```{r}
knitr::kable(honeymoon_period_longer)
```



##  {.smaller}

::: columns
::: {.column width="14.2%"}
![](fs.png)
:::

::: {.column width="14.2%"}
![](readr.png)
:::

::: {.column width="14.20%"}
![](tibble.png)
:::

::: {.column width="14.20%"}
![](pipe.png)
:::

::: {.column width="14.20%"}
![](dplyr.png)
:::

::: {.column width="14.20%"}
![](knitr.png)
:::

::: {.column width="14.20%"}
![](quarto.png)
:::
:::

::: columns
::: {.column width="17%"}
![](tidyverse.png)
:::

::: {.column width="71.5%"}

```
        READ -> WRANGLE -+-> TRANSFORM -+----+--> COMMUNICATE 
                         ^              |   /
                         |              v  /
                      MODEL <-- VISUALIZE_/
```

> Programs must be written for humans to read, and only incidentally for computers to execute.[^8]
:::
:::

[^8]: Abelson, H., Sussman, G. J. 1985. Structure and Interpretation of Computer Programs. Cambridge, MA: MIT Press. p.xxiv

::: columns
::: {.column width="14.20%"}
![](purrr.png)
:::

::: {.column width="14.20%"}
![](stringr.png)
:::

::: {.column width="14.20%"}
![](reticulate.png)
:::

::: {.column width="14.20%"}
![](ggplot2.png)
:::

::: {.column width="14.20%"}
![](tidyr.png)
:::

::: {.column width="14.20%"}
![](rmarkdown.png)
:::

::: {.column width="14.20%"}
![](shiny.png)
:::
:::

## Tidyverse Concepts

![](tidydata_1.jpg)

::: notes
When you organize data this way it becomes much easier to work with.

You can quickly address questions like:

-   How is this variable relate to another?
-   How do different values of this variable (or more than one variable) affect how another variable relates to another variable? (facets) (colors, sizes,shapes, text labels, etc.)
-   How can I arrange the data to answer a specific question?

The following apply to visualization and/or machine learning:

-   How can I best visualize the data to help answer a specific question?
-   How can I provide variables to a model to predict an outcome when the model requires input variables to be arranged in a certain way?
-   How can arrange the output of a model to be used in another model?
-   How can I

These tasks are all made easier by the structure of the data:

-   Data Quality /

-   Data Cleaning

    -   before and after you use a model to summarise or predict data
    -   even more important for machine learning to select variables (dimensionality reduction), normalize variables, or create new variables (feature engineering)

-   Efficient Data Handling: Working with large datasets can be challenging due to memory constraints and computational efficiency. Data science skills enable you to efficiently handle, process, and analyze large datasets, utilizing techniques such as batch processing, data streaming, and using efficient data structures.

-   Model Evaluation and Iteration: -After training models, you'll need to evaluate their performance and possibly iterate over the modeling process.

    -   Data manipulation skills are necessary to prepare validation datasets, perform cross-validation, and apply various metrics to assess model performance.
    -   Additionally, analyzing the errors your model makes can lead you back to data preparation steps to further refine your data.
:::

## Tidyverse Concepts

![](tidydata_2.jpg)

## Tidyverse Concepts

![](tidydata_3.jpg)

## Tidyverse Concepts

-   Many operations are essentially the same as SQL
    -   Algorithms are based on set theory, relational algebra, and relational calculus
    -   SQL is a declarative language, R and tidyverse R is an imperative/functional language
-   Methods for data wrangling are more flexible and powerful than SQL
-   Similar to SQL but without the focus on explicit predefined referencing and contraints of database design
-   Data Frames are the primary data structure

## Tabular Data in Python & Matlab {.scrollable}


::: columns
::: {.column width="50%"}

- Pandas in Python
   - Data Frames
      - Data Wrangling
         -   `read_csv()`
         -   `sort_values()`
         -   `sort_index()`
         -   `reset_index()`
         -   `set_index()`
         -   `loc[]`
         -   `iloc[]`
         -   `at[]`
         -   `iat[]`
         -   `isin()`
         -   `query()`
         -   `melt()`
         -   `stack()`
         -   `unstack()`
      - Split-Apply-Combine 
         -   `groupby()` 
         -   `pivot_table()`
      - Column-based computation 
         -   `eval()` 
         -   `assign()` 
         -   `pipe()` 
         -   `apply()` 
         -   `applymap()` 
         -   `map()`
      - DataFrame Joins 
         -   `merge()` 
         -   `join()` 
         -   `concat()`

:::

::: {.column width="50%"}

-   Data Tables in Matlab
    -   Data Tables
    -   Split-Apply-Combine
    -   Column-based computation
    -   Table Joins

:::

:::

::: notes
-   `cut()`, `qcut()`, `to_datetime()`, `to_numeric()`,
-   `to_timedelta()`, `str()`, `dt()`, `cat()`, `crosstab()`, `corr()`
-   `cov()`, `rolling()`, `expanding()`, `ewm()`, `shift()`, `diff()`,
-   `pct_change()`, `cumsum()`, `cumprod()`, `cummax()`, `cummin()`, `resample()`,
-   `asfreq()`, `interpolate()`, `fillna()`, `dropna()`, `replace()`,
:::

# How to Develop Coding Capabilities

## UF & Community Resources

[UF Research Computing](https://help.rc.ufl.edu/doc/UFRC_Help_and_Documentation)

[PracticumAI](https://practicumai.org/)

[Carpentries: R for Reproducible Scientific Analysis](https://swcarpentry.github.io/r-novice-gapminder/)

[Carpentries: Functional Neuroimaging Analysis in Python](https://carpentries-incubator.github.io/SDC-BIDS-fMRI/)

[DataCamp: Reconstructing Brain MRI Images Using Deep Learning (Convolutional Autoencoder)](https://www.datacamp.com/tutorial/reconstructing-brain-images-deep-learning)

[DataCamp: Introduction to Deep Learning with Keras](https://www.datacamp.com/courses/introduction-to-deep-learning-with-keras)

[DataCamp: How to Learn Deep Learning in 2024 - A Complete Guide](https://www.datacamp.com/blog/how-to-learn-deep-learning)


## Books, Online Courses, Tutorials {.smaller}

::: aside 

- Grisham, W., Abrams, M., Babiec, W. E., Fairhall, A. L., Kass, R. E., Wallisch, P., & Olivo, R. (2021). Teaching Computation in Neuroscience: Notes on the 2019 Society for Neuroscience Professional Development Workshop on Teaching. Journal of undergraduate neuroscience education : JUNE : a publication of FUN, Faculty for Undergraduate Neuroscience, 19(2), A185–A191. <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8437361>

- Ismay, C., & Kim, A. Y. (2019). Statistical inference via data science: a ModernDive into R and the tidyverse. Chapman and Hall/CRC. <https://moderndive.com/>

- Navarro, D. (2015). Learning statistics with R: A tutorial for psychology students and other beginners (version 0.6). University of New South Wales. <https://learningstatisticswithr.com/>

- Donoghue T, Voytek B, & Ellis S (2022). Course Materials for Data Science in Practice. Journal of Open Source Education, 5(51), 121. <https://doi.org/10.21105/jose.00121>

:::


## Other Opportunities

#### DataCamp, Coursera

#### Code Review & Collaboration

#### Mentorship & Coaching

#### Share & Teach

#### Hackathons & Competitions



## ChatGPT and Github Copilot {.smaller}

::: aside

- Gurdil, H., Soguksu, Y. B., Salihoglu, S., & Coskun, F. (2024). Integrating AI in Educational Measurement: ChatGPT's Efficacy in Item Response Theory Data Generation. arXiv preprint arXiv:2402.01731. <https://doi.org/10.48550/arXiv.2402.01731>

- Kashefi, A., & Mukerji, T. (2023). ChatGPT for programming numerical methods. Journal of Machine Learning for Modeling and Computing, 4(2). <https://doi.org/10.1615/JMachLearnModelComput.2023048492>

- Kim, D., Kim, T., Kim, Y., Byun, Y. H., & Yun, T. S. (2024). A ChatGPT-MATLAB framework for numerical modeling in geotechnical engineering applications. Computers and Geotechnics, 169, 106237. <https://doi.org/10.1016/j.compgeo.2024.106237>

- Kosar, T., Ostojić, D., Liu, Y. D., & Mernik, M. (2024). Computer Science Education in ChatGPT Era: Experiences from an Experiment in a Programming Course for Novice Programmers. Mathematics, 12(5), 629. <https://doi.org/10.3390/math12050629>

- Rahman, C. R., & Wong, L. (2023). How much can ChatGPT really help Computational Biologists in Programming?. arXiv preprint arXiv:2309.09126. <https://doi.org/10.48550/arXiv.2309.09126>

- Sänger, M., De Mecquenem, N., Lewińska, K. E., Bountris, V., Lehmann, F., Leser, U., & Kosch, T. (2023). Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT. arXiv preprint arXiv:2311.01825. <https://doi.org/10.48550/arXiv.2311.01825>

- Shen, Y., Ai, X., Soosai Raj, A. G., Leo John, R. J., & Syamkumar, M. (2024, March). Implications of ChatGPT for Data Science Education. In Proceedings of the 55th ACM Technical Symposium on Computer Science Education, 1, 1230-1236. <https://doi.org/10.1145/3626252.3630874>

- Silva, C. A. G. D., Ramos, F. N., de Moraes, R. V., & Santos, E. L. D. (2024). ChatGPT: Challenges and Benefits in Software Programming for Higher Education. Sustainability, 16(3), 1245. <https://doi.org/10.3390/su16031245>

- Waseem, M., Das, T., Ahmad, A., Fehmideh, M., Liang, P., & Mikkonen, T. (2023). Using ChatGPT throughout the Software Development Life Cycle by Novice Developers. arXiv preprint. <https://doi.org/10.48550/arXiv.2310.13648>

- White, J., Hays, S., Fu, Q., Spencer-Smith, J., & Schmidt, D. C. (2023). Chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design. arXiv preprint. <https://doi.org/10.48550/arXiv.2303.07839>
:::


## ChatGPT Coding Prompts {.smaller}

::: columns
::: {.column width="33%"}
**General coding workflows**

-   Code debugging
-   Code explanation
-   Code optimization
-   Code simplification
-   Code translation
-   Code quality and testing
    -   Compare function speeds
    -   Write unit tests
:::

::: {.column width="33%"}
**Analysis workflows**

-   Python/R analysis
    -   Data generation/cleaning
    -   Data analysis workflow in pandas/tidyverse R
    -   Data Aggregation
    -   Data Merging

**Data visualization**

-   Data visualization in Python/R
    -   Creating plots
    -   Annotating/formatting
    -   Changing plot themes
:::

::: {.column width="33%"}
**Machine learning workflows**

-   General machine learning
-   Python/R machine learning

**Time series analysis**

-   Python/R time series analysis

**Natural language processing workflows**

**Conceptual & career oriented prompts**
:::
:::



